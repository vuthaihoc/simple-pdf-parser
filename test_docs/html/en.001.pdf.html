<!DOCTYPE html>
<html lang="en-US">
<head>
<title>html from pdf</title>
<meta charset="utf-8">
<style>body{font-size: 18px;}</style>
<body>
<div data-page='1' style='padding: 20px;margin: 10px auto;max-width: 800px;'><p>Page 1</p><p><b>Similarity Estimation Techniques from Rounding</b>
<b>Algorithms</b>
</p>
<p>Moses S. Charikar
</p>
<p>Dept. of Computer Science
Princeton University
</p>
<p>35 Olden Street
Princeton, NJ 08544
</p>
<p>moses@cs.princeton.edu
</p>
<p><b>ABSTRACT</b>
</p>
<p>A locality sensitive hashing scheme is a distribution on a
familyF of hash functions operating on a collection of ob-
jects, such that for two objectsx, y,
</p>
<p>Prh∈F[h(x) =h(y)] =sim(x, y),
</p>
<p>wheresim(x, y)∈[0,1] is some similarity function defined
on the collection of objects. Such a scheme leads to a com-
pact representation of objects so that similarity of objects
can be estimated from their compact sketches, and also
leads to efficient algorithms for approximate nearest neigh-
bor search and clustering. Min-wise independent permu-
tations provide an elegant construction of such a locality
sensitive hashing scheme for a collection of subsets with the
set similarity measuresim(A, B) = |A∩B|<sub>|A∪B|</sub>.
</p>
<p>We show that rounding algorithms for LPs and SDPs used
in the context of approximation algorithms can be viewed
as locality sensitive hashing schemes for several interesting
collections of objects. Based on this insight, we construct
new locality sensitive hashing schemes for:
</p>
<p>1. A collection of vectors with the distance between ~u
and~v measured byθ(~u, ~v)/π, whereθ(~u, ~v) is the an-
gle between ~uand~v. This yields a sketching scheme
for estimating the cosine similarity measure between
two vectors, as well as a simple alternative to minwise
independent permutations for estimating set similar-
ity.
</p>
<p>2. A collection of distributions on n points in a metric
space, with distance between distributions measured
by the Earth Mover Distance (EMD), (a popular dis-
tance measure in graphics and vision). Our hash func-
tions map distributions to points in the metric space
such that, for distributionsP andQ,
</p>
<p>EMD(P, Q) ≤ Eh∈F[d(h(P), h(Q))]
</p>
<p>≤O(lognlog logn)·EMD(P, Q).
</p>
<p>Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
</p>
<p><i>STOC’02, May 19-21, 2002, Montreal, Quebec, Canada.</i>
Copyright 2002 ACM 1-58113-495-9/02/0005 ...$5.00.
</p>
<p><b>1.</b> <b>INTRODUCTION</b>
</p>
<p>The current information explosion has resulted in an in-
creasing number of applications that need to deal with large
volumes of data. While traditional algorithm analysis as-
sumes that the data fits in main memory, it is unreasonable
to make such assumptions when dealing with massive data
sets such as data from phone calls collected by phone com-
panies, multimedia data, web page repositories and so on.
This new setting has resulted in an increased interest in
algorithms that process the input data in restricted ways,
including sampling a few data points, making only a few
passes over the data, and constructing a succinct sketch of
the input which can then be efficiently processed.
</p>
<p>There has been a lot of recent work on streaming algo-
rithms, i.e. algorithms that produce an output by mak-
ing one pass (or a few passes) over the data while using a
limited amount of storage space and time. To cite a few
examples, Alonet al [2] considered the problem of estimat-
ing frequency moments and Guhaet al [25] considered the
problem of clustering points in a streaming fashion. Many
of these streaming algorithms need to represent important
aspects of the data they have seen so far in a small amount of
space; in other words they maintain a compact sketch of the
data that encapsulates the relevant properties of the data
set. Indeed, some of these techniques lead to sketching algo-
rithms – algorithms that produce a compact sketch of a data
set so that various measurements on the original data set
can be estimated by efficient computations on the compact
sketches. Building on the ideas of [2], Alonet al [1] give al-
gorithms for estimating join sizes. Gibbons and Matias [18]
give sketching algorithms producing so calledsynopsis data
structures for various problems including maintaining ap-
proximate histograms, hot lists and so on. Gilbertet al [19]
give algorithms to compute sketches for data streams so as
to estimate any linear projection of the data and use this to
get individual point and range estimates. Recently, Gilbert
et al [21] gave efficient algorithms for the dynamic mainte-
nance of histograms. Their algorithm processes a stream of
updates and maintains a small sketch of the data from which
the optimal histogram representation can be approximated
very quickly.
</p>
<p>In this work, we focus on sketching algorithms for estimat-
ing similarity, i.e. the construction of functions that produce
succinct sketches of objects in a collection, such that the
similarity of objects can be estimated efficiently from their
sketches. Here, similarity sim(x, y) is a function that maps
</p>
</div>
<div data-page='2' style='padding: 20px;margin: 10px auto;max-width: 800px;'><p>Page 2</p><p>pairs of objects x, y to a number in [0,1], measuring the
degree of similarity betweenxand y. sim(x, y) = 1 corre-
sponds to objectsx, ythat are identical whilesim(x, y) = 0
corresponds to objects that are very different.
</p>
<p>Broderet al [8, 5, 7, 6] introduced the notion ofmin-wise
independent permutations, a technique for constructing such
sketching functions for a collection of sets. The similarity
measure considered there was
</p>
<p>sim(A, B) = |A∩B|
|A∪B|.
</p>
<p>We note that this is exactly the Jaccard coefficient of simi-
larity used in information retrieval.
</p>
<p>The min-wise independent permutation scheme allows the
construction of a distribution on hash functionsh: 2U→U
such that
</p>
<p>Prh∈F[h(A) =h(B)] =sim(A, B).
</p>
<p>HereF denotes the family of hash functions (with an asso-
ciated probability distribution) operating on subsets of the
universeU. By choosing saythash functionsh1, . . . htfrom
</p>
<p>this family, a setS could be represented by the hash vector
(h1(S), . . . ht(S)). Now, the similarity between two sets can
</p>
<p>be estimated by counting the number of matching coordi-
nates in their corresponding hash vectors.1
</p>
<p>The work of Broderet alwas originally motivated by the
application of eliminating near-duplicate documents in the
Altavista index. Representing documents as sets of features
with similarity between sets determined as above, the hash-
ing technique provided a simple method for estimating sim-
ilarity of documents, thus allowing the original documents
to be discarded and reducing the input size significantly.
</p>
<p>In fact, the minwise independent permutations hashing
scheme is a particular instance of alocality sensitive hashing
schemeintroduced by Indyk and Motwani [31] in their work
on nearest neighbor search in high dimensions.
</p>
<p>Definition 1. A locality sensitive hashing scheme is a
distribution on a familyF of hash functions operating on a
collection of objects, such that for two objectsx, y,
</p>
<p>Prh∈F[h(x) =h(y)] =sim(x, y) (1)
</p>
<p>Here sim(x, y) is some similarity function defined on the
collection of objects.
</p>
<p>Given a hash function family F that satisfies (1), we will
say thatFis a locality sensitive hash function family corre-
sponding to similarity functionsim(x, y). Indyk and Mot-
wani showed that such a hashing scheme facilitates the con-
struction of efficient data structures for answering approxi-
mate nearest-neighbor queries on the collection of objects.
</p>
<p>In particular, using the hashing scheme given by minwise
independent permutations results in efficient data structures
for set similarity queries and leads to efficient clustering al-
gorithms. This was exploited later in several experimental
papers: Cohenet al [14] for association-rule mining, Haveli-
walaet al [27] for clustering web documents, Chenet al [13]
for selectivity estimation of boolean queries, Chenet al [12]
for twig queries, and Gioniset al [22] for indexing set value
</p>
<p>1<sub>One question left open in [7] was the issue of compact rep-</sub>
</p>
<p>resentation of hash functions in this family; this was settled
by Indyk [28], who gave a construction of a small family of
minwise independent permutations.
</p>
<p>attributes. All of this work used the hashing technique for
set similarity together with ideas from [31].
</p>
<p>We note that the definition of locality sensitive hashing
used by [31] is slightly different, although in the same spirit
as our definition. Their definition involves parametersr1&gt;
</p>
<p>r2 and p1 &gt; p2. A family F is said to be (r1, r2, p1, p2)-
</p>
<p>sensitive for a similarity measuresim(x, y) ifPrh∈F[h(x) =
</p>
<p>h(y)]≥p1 whensim(x, y)≥r1 andPrh∈F[h(x) =h(y)]≤
</p>
<p>p2 whensim(x, y) ≤r2. Despite the difference in the pre-
</p>
<p>cise definition, we chose to retain the namelocality sensitive
hashingin this work since the two notions are essentially the
same. Hash functions with closely related properties were
investigated earlier by Linial and Sasson [34] and Indyket
al[32].
</p>
<p><b>1.1</b> <b>Our Results</b>
</p>
<p>In this paper, we explore constructions of locality sensi-
tive hash functions for various other interesting similarity
functions. The utility of such hash function schemes (for
nearest neighbor queries and clustering) crucially depends
on the fact that the similarity estimation is based on a test
of equality of the hash function values. We make an interest-
ing connection between constructions of similarity preserv-
ing hash-functions and rounding procedures used in the de-
sign of approximation algorithms. We show that procedures
used for rounding fractional solutions from linear programs
and vector solutions to semidefinite programs can be used
to derive similarity preserving hash functions for interesting
classes of similarity functions.
</p>
<p>In Section 2, we prove some necessary conditions on sim-
ilarity measuressim(x, y) for the existence of locality sensi-
tive hash functions satisfying (1). Using this, we show that
such locality sensitive hash functions do not exist for certain
commonly used similarity measures in information retrieval,
the Dice coefficient and the Overlap coefficient.
</p>
<p>In seminal work, Goemans and Williamson [24] intro-
duced semidefinite programming relaxations as a tool for
approximation algorithms. They used the random hyper-
plane rounding technique to round vector solutions for the
MAX-CUT problem. We will see in Section 3 that the ran-
dom hyperplane technique naturally gives a family of hash
functionsF for vectors such that
</p>
<p>Prh∈F[h(~u) =h(~v)] = 1−
</p>
<p>θ(~u, ~v)
π .
</p>
<p>Here θ(~u, ~v) refers to the angle between vectors ~u and ~v.
Note that the function 1−θ
</p>
<p>π is closely related to the func-
</p>
<p>tioncos(θ). (In fact it is always within a factor 0.878 from it.
Moreover, cos(θ) can be estimated from an estimate of θ.)
Thus this similarity function is very closely related to the
cosine similarity measure, commonly used in information
retrieval. (In fact, Indyk and Motwani [31] describe how
the set similarity measure can be adapted to measure dot
product between binary vectors ind-dimensional Hamming
space. Their approach breaks up the data set intoO(logd)
groups, each consisting of approximately the same weight.
Our approach, based on estimating the angle between vec-
tors is more direct and is also more general since it applies
to general vectors.) We also note that the cosine between
vectors can be estimated from known techniques based on
random projections [2, 1, 20]. However, the advantage of a
locality sensitive hashing based scheme is that this directly
yields techniques for nearest neighbor search for the cosine
similarity measure.
</p>
</div>
<div data-page='3' style='padding: 20px;margin: 10px auto;max-width: 800px;'><p>Page 3</p><p>An attractive feature of the hash functions obtained from
the random hyperplane method is that the output is a single
bit; thus the output ofthash functions can be concatenated
very easily to produce at-bit vector.2 Estimating similarity
between vectors amounts to measuring the Hamming dis-
tance between the correspondingt-bit hash vectors. We can
represent sets by their characteristic vectors and use this
locality sensitive hashing scheme for measuring similarity
between sets. This yields a slightly different similarity mea-
sure for sets, one that is linearly proportional to the angle
between their characteristic vectors.
</p>
<p>In Section 4, we present a locality sensitive hashing scheme
for a certain metric on distributions on points, called the
Earth Mover Distance. We are given a set of points L =
{l1, . . . ln}, with a distance functiond(i, j) defined on them.
</p>
<p>A probability distributionP(X) (or distribution for short) is
a set of weightsp1, . . . pnon the points such thatpi≥0 and
</p>
<p>pi= 1. (We will often refer to distributionP(X) as sim-
</p>
<p>plyP, implicitly referring to an underlying setXof points.)
The Earth Mover Distance EMD(P, Q) between two dis-
tributions P and Q is defined to be the cost of the min
cost matching that transforms one distribution to another.
(Imagine each distribution as placing a certain amount of
earth on each point. EMD(P, Q) measures the minimum
amount of work that must be done in transforming one dis-
tribution to the other.) This is a popular metric for images
and is used for image similarity, navigating image databases
and so on [37, 38, 39, 40, 36, 15, 16, 41, 42]. The idea
is to represent an image as a distribution on features with
an underlying distance metric on features (e.g. colors in a
color spectrum). Since the earth mover distance is expensive
to compute (requiring a solution to a minimum transporta-
tion problem), applications typically use an approximation
of the earth mover distance. (e.g. representing distributions
by their centroids).
</p>
<p>We construct a hash function family for estimating the
earth mover distance. Our family is based on rounding al-
gorithms for LP relaxations for the problem of classification
with pairwise relationships studied by Kleinberg and Tar-
dos [33], and further studied by Calinescu et al [10] and
Chekuriet al [11]. Combining a new LP formulation de-
scribed by Chekuriet altogether with a rounding technique
of Kleinberg and Tardos, we show a construction of a hash
function family which approximates the earth mover dis-
tance to a factor ofO(lognlog logn). Each hash function in
this family maps a distribution on pointsL = {l1, . . . , ln}
</p>
<p>to some pointliin the set. For two distributionsP(X) and
</p>
<p>Q(X) on the set of points, our family of hash functions F
satisfies the property that:
</p>
<p>EMD(P, Q) ≤ Eh∈F[d(h(P), h(Q))]
</p>
<p>≤O(lognlog logn)·EMD(P, Q).
We also show an interesting fact about a rounding al-
gorithm in Kleinberg and Tardos [33] applying to the case
where the underlying metric on points is a uniform met-
ric. In this case, we show that their rounding algorithm can
</p>
<p>2<sub>In Section 2, we will show that we can convert any locality</sub>
</p>
<p>sensitive hashing scheme to one that maps objects to{0,1}
with a slight change in similarity measure. However, the
modified hash functions convey less information, e.g. the
collision probability for the modified hash function family is
at least 1/2 even for a pair of objects with original similarity
0.
</p>
<p>be viewed as a generalization of min-wise independent per-
mutations extended to a continuous setting. Their rounding
procedure yields a locality sensitive hash function for vectors
whose coordinates are all non-negative. Given two vectors
~a = (a1, . . . an) and~b= (b1, . . . bn), the similarity function
</p>
<p>is
</p>
<p>sim(~a,~b) = imin(ai, bi)
imax(ai, bi)
</p>
<p>.
</p>
<p>(Note that when~a and~b are the characteristic vectors for
sets A andB, this expression reduces to the set similarity
measure for min-wise independent permutations.)
</p>
<p>Applications of locality sensitive hash functions to solving
nearest neighbor queries typically reduce the problem to the
Hamming space. Indyk and Motwani [31] give a data struc-
ture that solves the approximate nearest neighbor problem
on the Hamming space. Their construction is a reduction to
the so called PLEB (Point Location in Equal Balls) problem,
followed by a hashing technique concatenating the values of
several locality sensitive hash functions. We give a simple
technique that achieves the same performance as the Indyk
Motwani result in Section 5. The basic idea is as follows:
Given bit vectors consisting ofdbits each, we choose a num-
ber of random permutations of the bits. For each random
permutationσ, we maintain a sorted order of the bit vectors,
in lexicographic order of the bits permuted byσ. To find a
nearest neighbor for a query bit vectorq we do the follow-
ing: For each permutationσ, we perform a binary search on
the sorted order corresponding toσto locate the bit vectors
closest toq(in the lexicographic order obtained by bits per-
muted byσ). Further, we search in each of the sorted orders
proceeding upwards and downwards from the location ofq,
according to a certain rule. Of all the bit vectors examined,
we return the one that has the smallest Hamming distance
to the query vector. The performance bounds we can prove
for this simple scheme are identical to that proved by Indyk
and Motwani for their scheme.
</p>
<p><b>2.</b> <b>EXISTENCE OF LOCALITY SENSITIVE</b>
<b>HASH FUNCTIONS</b>
</p>
<p>In this section, we discuss certain necessary properties for
the existence of locality sensitive hash function families for
given similarity measures.
</p>
<p>Lemma 1. For any similarity functionsim(x, y)that ad-
mits a locality sensitive hash function family as defined in
(1), the distance function1−sim(x, y)satisfies triangle in-
equality.
</p>
<p>Proof. Suppose there exists a locality sensitive hash func-
tion family such that
</p>
<p>Prh∈F[h(x) =h(y)] =sim(x, y).
</p>
<p>Then,
</p>
<p>1−sim(x, y) =Prh∈F[h(x)6=h(y)].
</p>
<p>Let ∆h(x, y) be an indicator variable for the eventh(x)6=
</p>
<p>h(y). We claim that ∆h(x, y) satisfies the triangle inequality,
</p>
<p>i.e.
</p>
<p>∆h(x, y) + ∆h(y, z)≥∆h(x, z).
</p>
</div>
<div data-page='4' style='padding: 20px;margin: 10px auto;max-width: 800px;'><p>Page 4</p><p>Since ∆h() takes values in the set {0,1}, the only case
</p>
<p>when the above inequality could be violated would be when
∆h(x, y) = ∆h(y, z) = 0. But in this caseh(x) =h(y) and
</p>
<p>h(y) =h(z). Thus,h(x) =h(z) implying that ∆h(x, z) = 0
</p>
<p>and the inequality is satisfied. This proves the claim. Now,
1−sim(x, y) =Eh∈F[∆h(x, y)]
</p>
<p>Since ∆h(x, y) satisfies the triangle inequality,Eh∈F[∆h(x, y)]
</p>
<p>must also satisfy the triangle inequality. This proves the
lemma.
</p>
<p>This gives a very simple proof of the fact that for the
set similarity measure sim(A, B) = |A∩B|<sub>|A∪B|</sub>, 1−sim(A, B)
satisfies the triangle inequality. This follows from Lemma 1
and the fact that a set similarity measure admits a locality
sensitive hash function family, namely that given by minwise
independent permutations.
</p>
<p>One could ask the question whether locality sensitive hash
functions satisfying the definition (1) exist for other com-
monly used set similarity measures in information retrieval.
For example, Dice’s coefficient is defined as
</p>
<p>sim<sub>D</sub><sub>ice</sub>(A, B) = <sub>1</sub> |A∩B|
</p>
<p>2(|A|+|B|)
</p>
<p>The Overlap coefficient is defined as
</p>
<p>sim<sub>O</sub><sub>vl</sub>(A, B) = |A∩B|
min(|A|,|B|)
</p>
<p>We can use Lemma 1 to show that there is no such local-
ity sensitive hash function family for Dice’s coefficient and
the Overlap measure by showing that the corresponding dis-
tance function does not satisfy triangle inequality.
</p>
<p>Consider the setsA={a}, B={b}, C={a, b}. Then,
</p>
<p>sim<sub>D</sub><sub>ice</sub>(A, C) = 2
</p>
<p>3, simDice(C, B) =
</p>
<p>2
3,
sim<sub>D</sub><sub>ice</sub>(A, B) = 0
</p>
<p>1−sim<sub>D</sub><sub>ice</sub>(A, C) + 1−sim<sub>D</sub><sub>ice</sub>(C, B)
&lt; 1−sim<sub>D</sub><sub>ice</sub>(A, B)
Similarly, the values for the Overlap measure are as follows:
sim<sub>O</sub><sub>vl</sub>(A, C) = 1, sim<sub>O</sub><sub>vl</sub>(C, B) = 1, sim<sub>O</sub><sub>vl</sub>(A, B) = 0
1−sim<sub>O</sub><sub>vl</sub>(A, C) + 1−sim<sub>O</sub><sub>vl</sub>(C, B)&lt;1−sim<sub>O</sub><sub>vl</sub>(A, B)
This shows that there is no locality sensitive hash function
family corresponding to Dice’s coefficient and the Overlap
measure.
</p>
<p>It is often convenient to have a hash function family that
maps objects to{0,1}. In that case, the output oftdifferent
hash functions can simply be concatenated to obtain at-bit
hash value for an object. In fact, we can always obtain such
a binary hash function family with a slight change in the
similarity measure. A similar result was used and proved by
Gioniset al[22]. We include a proof for completeness.
</p>
<p>Lemma 2. Given a locality sensitive hash function family
F corresponding to a similarity function sim(x, y), we can
obtain a locality sensitive hash function familyF0 <sub>that maps</sub>
</p>
<p>objects to {0,1} and corresponds to the similarity function
</p>
<p>1+sim(x,y)
</p>
<p>2 .
</p>
<p>Proof. Suppose we have a hash function family such
that
</p>
<p>Prh∈F[h(x) =h(y)] =sim(x, y).
</p>
<p>Let B be a pairwise independent family of hash functions
that operate on the domain of the functions inFand map el-
ements in the domain to{0,1}. ThenPrb∈B[b(u) =b(v)] =
</p>
<p>1/2 ifu6=vandPrb∈B[b(u) =b(v)] = 1 ifu=v. Consider
</p>
<p>the hash function family obtained by composing a hash func-
tion fromF with one fromB. This maps objects to{0,1}
and we claim that it has the required properties.
</p>
<p>Prh∈F,b∈B[b(h(x)) =b(h(y))] =
</p>
<p>1 +sim(x, y)
2
</p>
<p>With probabilitysim(x, y),h(x) =h(y) and henceb(h(x) =
b(h(y)). With probability 1−sim(x, y),h(x)6=h(y) and in
this case,Prb∈B[b(h(x) =b(h(y))] = 12. Thus,
</p>
<p>Pr[b(h(x)) =b(h(y))] = sim(x, y) + (1−sim(x, y))/2
= (1 +sim(x, y))/2.
</p>
<p>This can be used to show a stronger condition for the
existence of a locality sensitive hash function family.
</p>
<p>Lemma 3. For any similarity functionsim(x, y)that ad-
mits a locality sensitive hash function family as defined in
(1), the distance function1−sim(x, y)is isometrically em-
beddable in the Hamming cube.
</p>
<p>Proof. Firstly, we apply Lemma 2 to construct a binary
locality sensitive hash function family corresponding to sim-
ilarity function sim0<sub>(x, y) = (1 +</sub><sub>sim(x, y))/2. Note that</sub>
</p>
<p>such a binary hash function family gives an embedding of
objects into the Hamming cube (obtained by concatenating
the values of all the hash functions in the family). For ob-
ject x, let v(x) be the element in the Hamming cube x is
mapped to. 1−sim0<sub>(x, y) is simply the fraction of bits that</sub>
</p>
<p>do not agree inv(x) andv(y), which is proportional to the
Hamming distance between v(x) and v(y). Thus this em-
bedding is an isometric embedding of the distance function
1−sim0<sub>(x, y) in the Hamming cube. But</sub>
</p>
<p>1−sim0(x, y) = 1−(1 +sim(x, y))/2 = (1−sim(x, y))/2.
This implies that 1−sim(x, y) can be isometrically embed-
ded in the Hamming cube.
</p>
<p>We note that Lemma 3 has a weak converse, i.e. for a
similarity measuresim(x, y) any isometric embedding of the
distance function 1−sim(x, y) in the Hamming cube yields a
locality sensitive hash function family corresponding to the
similarity measure (α+sim(x, y))/(α+ 1) for someα &gt;0.
</p>
<p><b>3.</b> <b>RANDOM HYPERPLANE BASED HASH</b>
<b>FUNCTIONS FOR VECTORS</b>
</p>
<p>Given a collection of vectors inRd<sub>, we consider the family</sub>
</p>
<p>of hash functions defined as follows: We choose a random
vector~rfrom thed-dimensional Gaussian distribution (i.e.
each coordinate is drawn the 1-dimensional Gaussian distri-
bution). Corresponding to this vector ~r, we define a hash
</p>
</div>
<div data-page='5' style='padding: 20px;margin: 10px auto;max-width: 800px;'><p>Page 5</p><p>functionh~r as follows:
</p>
<p>hr~(~u) = 1<sub>0</sub> if<sub>if</sub><sub>~r</sub>~r·<sub>·</sub><sub>u &lt;</sub><sub>~</sub>u~≥0<sub>0</sub>
</p>
<p>Then for vectors~uand~v,
</p>
<p>Pr[h~r(~u) =h~r(~v)] = 1−
</p>
<p>θ(~u, ~v)
π .
</p>
<p>This was used by Goemans and Williamson [24] in their
rounding scheme for the semidefinite programming relax-
ation of MAX-CUT.
</p>
<p>Picking a random hyperplane amounts to choosing a nor-
mally distributed random variable for each dimension. Thus
even representing a hash function in this family could require
a large number of random bits. However, fornvectors, the
hash functions can be chosen by pickingO(log2<sub>n) random</sub>
</p>
<p>bits, i.e. we can restrict the random hyperplanes to be in
a family of size 2O(log2<sub>n)</sub>
</p>
<p>. This follows from the techniques
in Indyk [30] and Engebretsenet al [17], which in turn use
Nisan’s pseudorandom number generator for space bounded
computations [35]. We omit the details since they are similar
to those in [30, 17].
</p>
<p>Using this random hyperplane based hash function, we ob-
tain a hash function family for set similarity, for a slightly
different measure of similarity of sets. Suppose sets are rep-
resented by their characteristic vectors. Then, applying the
above scheme gives a locality sensitive hashing scheme where
</p>
<p>Pr[h(A) =h(B)] = 1− θ
π, where
θ = cos−1  |A∩B|
</p>
<p>
</p>
<p>|A| · |B|
</p>
<p>Also, this hash function family facilitates easy incorporation
of element weights in the similarity calculation, since the
values of the coordinates of the characteristic vectors could
be real valued element weights. Later, in Section 4.1 we will
present another technique to define and estimate similarity
of weighted sets.
</p>
<p><b>4.</b> <b>THE EARTH MOVER DISTANCE</b>
</p>
<p>Consider a set of points L= {l1, . . . ln} with a distance
</p>
<p>function d(i, j) (assumed to be a metric). A distribution
P(L) onLis a collection of non-negative weights (p1, . . . pn)
</p>
<p>for points inX such that pi = 1. The distance between
</p>
<p>two distributionsP(L) andQ(L) is defined to be the optimal
cost of the following minimum transportation problem:
</p>
<p>min
</p>
<p>i,j
</p>
<p>fi,j·d(i, j) (2)
</p>
<p>∀i 
</p>
<p>j
</p>
<p>fi,j = pi (3)
</p>
<p>∀j 
</p>
<p>i
</p>
<p>fi,j = qj (4)
</p>
<p>∀i, j fi,j ≥ 0 (5)
</p>
<p>Note that we define a somewhat restricted form of the
Earth Mover Distance. The general definition does not as-
sume that the sum of the weights is identical for distribu-
tionsP(L) andQ(L). This is useful for example in matching
a small image to a portion of a larger image.
</p>
<p>We will construct a hash function family for estimating
the Earth Mover Distance based on rounding algorithms for
the problem of classification with pairwise relationships, in-
troduced by Kleinberg and Tardos [33]. (A closely related
problem was also studied by Broderet al [9]). In designing
hash functions to estimate the Earth Mover Distance, we
will relax the definition of locality sensitive hashing (1) in
three ways.
</p>
<p>1. Firstly, the quantity we are trying to estimate is a
distance measure, not a similarity measure in [0,1].
2. Secondly, we will allow the hash functions to map ob-
</p>
<p>jects to points in a metric space and measure
</p>
<p>E[d(h(x), h(y))]. (A locality sensitive hash function
for a similarity measure sim(x, y) can be viewed as
a scheme to estimate the distance 1−sim(x, y) by
Prh∈F[h(x) 6= h(y)]. This is equivalent to having a
</p>
<p>uniform metric on the hash values).
</p>
<p>3. Thirdly, our estimator for the Earth Mover Distance
will not be an unbiased estimator, i.e. our estimate
will approximate the Earth Mover Distance to within
a small factor.
</p>
<p>We now describe the problem of classification with pair-
wise relationships. Given a collection of objectsV and labels
L={l1, . . . , ln}, the goal is to assign labels to objects. The
</p>
<p>cost of assigning label lto objectu∈V is c(u, l). Certain
pairs of objects (u, v) are related; such pairs form the edges
of a graph overV. Each edgee= (u, v) is associated with a
non-negative weightwe. For edgee= (u, v), ifuis assigned
</p>
<p>labelh(u) andvis assigned labelh(v), then the cost paid is
wed(h(u), h(v)).
</p>
<p>The problem is to come up with an assignment of labels
h:V →L, so as to minimize the cost of the labelinghgiven
by
</p>
<p>
</p>
<p>u∈V
</p>
<p>c(v, h(v)) + 
</p>
<p>e=(u,v)∈E
</p>
<p>wed(h(u), h(v))
</p>
<p>The approximation algorithms for this problem use an LP
to assign, for every u ∈V, a probability distribution over
labels inL (i.e. a set of non-negative weights that sum up
to 1). Given a distribution P over labels inL, the round-
ing algorithm of Kleinberg and Tardos gave a randomized
procedure for assigning label h(P) toP with the following
properties:
</p>
<p>1. Given distributionP(L) = (p1, . . . pn),
</p>
<p>Pr[h(P) =li] =pi. (6)
</p>
<p>2. SupposeP andQare probability distributions overL.
E[d(h(P), h(Q))]≤O(lognlog logn)EMD(P, Q) (7)
We note that the second property (7) is not immediately
obvious from [33], since they do not describe LP relaxations
for general metrics. Their LP relaxations are defined for
Hierarchically well Separated Trees (HSTs). They convert
a general metric to such an HST using Bartal’s results [3,
4] on probabilistic approximation of metric spaces via tree
metrics. However, it follows from combining ideas in [33]
with those in Chekuriet al [11]. Chekuriet al do in fact
give an LP relaxation for general metrics. The LP relaxation
</p>
</div>
<div data-page='6' style='padding: 20px;margin: 10px auto;max-width: 800px;'><p>Page 6</p><p>does indeed produce distributions over labels for every ob-
jectu∈V. Thefractional distancebetween two labelings is
expressed as the min cost transshipment betweenP andQ,
which is identical to the Earth Mover DistanceEMD(P, Q).
Now, this fractional solution can be used in the rounding al-
gorithm developed by Kleinberg and Tardos to obtain the
second property (7) claimed above. In fact, Chekuri et al
use this fact to claim that the gap of their LP relaxation is
at mostO(lognlog logn) (Theorem 5.1 in [11]).
</p>
<p>We elaborate some more on why the property (7) holds.
Kleinberg and Tardos first (probabilistically) approximate
the metric on L by an HST using [3, 4]. This is a tree
with all vertices in the original metric at the leaves. The
pairwise distance between any two vertices does no decrease
and all pairwise distances are increased by a factor of at
mostO(lognlog logn) (in expectation). For this tree met-
ric, they use an LP formulation which can be described as
follows. Suppose we have a rooted tree. For subtreeT, let
`T denote the length of the edge thatT hangs off of, i.e. the
</p>
<p>first edge on the path fromT to the root. Further, for dis-
tributionP on the vertices of the original metric, letP(T)
denote the total probability mass thatP assigns to leaves in
T;Q(T) is similarly defined. The distance between distribu-
tionsP andQis measured by <sub>T</sub>`T|P(T)−Q(T)|, where
</p>
<p>the summation is computed over all subtreesT. The Klein-
berg Tardos rounding scheme ensures thatE[d(h(P), h(Q))]
is within a constant factor of <sub>T</sub>`T|P(T)−Q(T)|.
</p>
<p>Suppose instead, we measured the distance between distri-
butions byEMD(P, Q), defined on the original metric. By
probabilistically approximating the original metric by a tree
metricT0<sub>, the expected value of the distance</sub><sub>EMD</sub>
</p>
<p>T0(P, Q)
</p>
<p>(on the tree metricT0<sub>) is at most a factor of</sub><sub>O(log</sub><sub>n</sub><sub>log log</sub><sub>n)</sub>
</p>
<p>timesEMD(P, Q). This follows since all distances increase
byO(lognlog logn) in expectation. Now note that the tree
distance measure used by Kleinberg and Tardos <sub>T</sub>`T|P(T)−
</p>
<p>Q(T)| is a lower bound on (and in fact exactly equal to)
EMDT0(P, Q). To see that this is a lower bound, note that
</p>
<p>in the min cost transportation between P and Q on T0<sub>,</sub>
</p>
<p>the flow on the edge leading upwards from subtreeT must
be at least|P(T)−Q(T)|. Since the rounding scheme en-
sures that E[d(h(P), h(Q))] is within a constant factor of
</p>
<p>T`T|P(T)−Q(T)|, we have that
</p>
<p>E[d(h(P), h(Q))] ≤ O(1)EMDT0(P, Q)
</p>
<p>≤ O(lognlog logn)EMD(P, Q)
where the expectation is over the random choice of the HST
and the random choices made by the rounding procedure.
</p>
<p>Theorem 1. The Kleinberg Tardos rounding scheme yields
a locality sensitive hashing scheme such that
</p>
<p>EMD(P, Q) ≤ E[d(h(P), h(Q))]
</p>
<p>≤O(lognlog logn)EMD(P, Q).
</p>
<p>Proof. The upper bound onE[d(h(P), h(Q))] follows di-
rectly from the second property (7) of the rounding scheme
stated above.
</p>
<p>We show that the lower bound follows from the first prop-
erty (6). Let yi,j be the joint probability that h(P) = li
</p>
<p>and h(Q) =lj. Note that jyi,j =pi, since this is sim-
</p>
<p>ply the probability thath(P) =li. Similarly <sub>i</sub>yi,j =qj,
</p>
<p>since this is simply the probability thath(Q) =lj. Now, if
</p>
<p>h(P) =liandh(Q) =lj, thend(h(P)h(Q)) =d(i, j). Hence
</p>
<p>E[d(f(P), f(Q))] = <sub>i,j</sub>yi,j·d(i, j). Let us write down the
</p>
<p>expected cost and the constraints onyi,j.
</p>
<p>E[d(h(P), h(Q))] = 
</p>
<p>i,j
</p>
<p>yi,j·d(i, j)
</p>
<p>∀i 
</p>
<p>j
</p>
<p>yi,j = pi
</p>
<p>∀j 
</p>
<p>i
</p>
<p>yi,j = qj
</p>
<p>∀i, j yi,j ≥ 0
</p>
<p>Comparing this with the LP for EMD(P, Q), we see that
the values offi,j=yi,jis a feasible solution to the LP (2) to
</p>
<p>(5) andE[d(h(P), h(Q))] is exactly the value of this solution.
SinceEMD(P, Q) is the minimum value of a feasible solu-
tion, it follows thatEMD(P, Q)≤E[d(h(P), h(Q))].
</p>
<p>Calinescu et al [10] study a variant of the classification
problem with pairwise relationships called the 0-extension
problem. This is the version without assignment costs where
some objects are assigned labels apriori and this labeling
must be extended to the other objects (a generalization of
multiway cut). For this problem, they design a rounding
scheme to get aO(logn) approximation. Again, their tech-
nique does not explicitly use an LP that gives probability
distributions on labels. However in hindsight, their round-
ing scheme can be interpreted as a randomized procedure
for assigning labels to distributions such that
</p>
<p>E[d(h(P), h(Q))]≤O(logn)EMD(P, Q).
</p>
<p>Thus their rounding scheme gives a tighter guarantee than
(7). However, they do not ensure (6). Thus the previous
proof showing that EMD(P, Q) ≤ E[d(h(P), h(Q))] does
not apply. In fact one can construct examples such that
EMD(P, Q) &gt; 0, yet E[d(h(P), h(Q))] = 0. Hence, the
resulting hash function family provides an upper bound on
EMD(P, Q) within a factorO(logn) but does not provide
a good lower bound.
</p>
<p>We mention that the hashing scheme described provides
an approximation to the Earth Mover Distance where the
quality of the approximation is exactly the factor by which
the underlying metric can be probabilistically approximated
by HSTs. In particular, if the underlying metric itself is
an HST, this yields an estimate within a constant factor.
This could have applications in compactly representing dis-
tributions over hierarchical classes. For example, documents
can be assigned a probability distribution over classes in
the Open Directory Project (ODP) hierarchy. This hier-
archy could be thought of as an HST and documents can
be mapped to distributions over this HST. The distance be-
tween two distributions can be measured by the Earth Mover
Distance. In this case, the hashing scheme described gives a
way to estimate this distance measure to within a constant
factor.
</p>
<p><b>4.1</b> <b>Weighted Sets</b>
</p>
<p>We show that the Kleinberg Tardos [33] rounding scheme
for the case of the uniform metric actually is an extension
of min-wise independent permutations to the weighted case.
First we recall the hashing scheme given by min-wise in-
dependent permutations. Given a universe U, consider a
random permutationπ ofU. Assume that the elements of
U are totally ordered. Given a subset A ⊆ U, the hash
</p>
</div>
<div data-page='7' style='padding: 20px;margin: 10px auto;max-width: 800px;'><p>Page 7</p><p>functionhπis defined as follows:
</p>
<p>hπ(A) = min{π(A)}
</p>
<p>Then the property satisfied by this hash function family is
that
</p>
<p>Prπ[hπ(A) =hπ(B)] =
</p>
<p>|A∩B|
|A∪B|
</p>
<p>We now review the Kleinberg Tardos rounding scheme for
the uniform metric: Firstly, imagine that we pick an infinite
sequence{(it, αt)}∞t=1where for eacht,itis picked uniformly
</p>
<p>and at random in{1, . . . n}andαt is picked uniformly and
</p>
<p>at random in [0,1]. Given a distribution P = (p1, . . . , pn),
</p>
<p>the assignment of labels is done in phases. In theith phase,
we check whetherαi≤pit. If this is the case andP has not
</p>
<p>been assigned a label yet, it is assigned labelit.
</p>
<p>Now, we can think of these distributions as sets inR2(see
Figure 1).
</p>
<p>2 3 4 5 6 7
</p>
<p>0 1 8
</p>
<p>Figure 1: Viewing a distribution as a continuous set.
</p>
<p>The setS(P) corresponding to distributionP consists of
the union of the rectangles [i−1, i]×[0, pi]. The elements of
</p>
<p>the universe are [i−1, i]×α. [i−1, i]×αbelongs toS(P)
iffα≤pi. The notion ofcardinality of union and intersec-
</p>
<p>tion of sets is replaced by theareaof the intersection and
union of two such sets inR2. Note that the Kleinberg Tar-
dos rounding scheme can be interpreted as constructing a
permutation of the universe and assigning to a distribution
P, the value isuch that (i, α) is the minimum in the per-
mutation amongst all elements contained inS(P). Suppose
instead, we assign toP, the element (i, α) which is the min-
imum in the permutation ofS(P). Lethbe a hash function
derived from this scheme (a slight modification of the one in
[33]). Then,
</p>
<p>Pr[h(P) =h(Q)] =|S(P)∩S(Q)|
|S(P)∪S(Q)| =
</p>
<p>imin(pi, qi)
imax(pi, qi)
</p>
<p>(8)
For the Kleinberg Tardos rounding scheme, the probabil-
ity of collision is at least the probability of collision for the
modified scheme (since two objects hashed to (i, α1) and
</p>
<p>(i, α2) respectively in the modified scheme would be both
</p>
<p>mapped toiin the original scheme). Hence
</p>
<p>PrKT[h(P) =h(Q)] ≥ i
</p>
<p>min(pi, qi)
imax(pi, qi)
</p>
<p>PrKT[h(P)6=h(Q)] ≤ 1− i
</p>
<p>min(pi, qi)
imax(pi, qi)
</p>
<p>= i|pi−qi|
imax(pi, qi)
</p>
<p>≤
</p>
<p>i
</p>
<p>|pi−qi|
</p>
<p>The last inequality follows from the fact that pi= qi=
</p>
<p>1 in the Kleinberg Tardos setting. This was exactly the
property used in [33] to obtain a 2-approximation for the
uniform metric case.
</p>
<p>Note that the hashing scheme given by (8) is a generaliza-
tion of min-wise independent permutations to the weighted
setting where elements in sets are associated with weights
∈ [0,1]. Min-wise independent permutations are a special
case of this scheme when the weights are{0,1}. This scheme
could be useful in a setting where a weighted set similar-
ity notion is desired. We note that the original min-wise
independent permutations can be used in the setting of in-
teger weights by simply duplicating elements according to
their weight. The present scheme would work for any non-
negative real weights.
</p>
<p><b>5.</b> <b>APPROXIMATE NEAREST NEIGHBOR</b>
<b>SEARCH IN HAMMING SPACE.</b>
</p>
<p>Applications of locality sensitive hash functions to solving
nearest neighbor queries typically reduce the problem to the
Hamming space. Indyk and Motwani [31] give a data struc-
ture that solves the approximate nearest neighbor problem
on the Hamming space Hd. Their construction is a reduc-
tion to the so called PLEB (Point Location in Equal Balls)
problem, followed by a hashing technique concatenating the
values of several locality sensitive hash functions.
</p>
<p>Theorem 2 ([31]). For any  &gt;0, there exists an al-
gorithm for -PLEB inHd<sub>using</sub> <sub>O(dn</sub><sub>+</sub><sub>n</sub>1+1/(1+)<sub>)</sub> <sub>space</sub>
</p>
<p>andO(n1/(1+)<sub>)</sub><sub>hash function evaluations for each query.</sub>
</p>
<p>We give a simple technique that achieves the same perfor-
mance as the Indyk Motwani result:
</p>
<p>Given bit vectors consisting of d bits each, we choose
N =O(n1/(1+)<sub>) random permutations of the bits. For each</sub>
</p>
<p>random permutation σ, we maintain a sorted order Oσ of
</p>
<p>the bit vectors, in lexicographic order of the bits permuted
byσ. Given a query bit vector q, we find the approximate
nearest neighbor by doing the following: For each permu-
tation σ, we perform a binary search onOσ to locate the
</p>
<p>two bit vectors closest to q (in the lexicographic order ob-
tained by bits permuted by σ). We now search in each of
the sorted ordersOσ examining elements above and below
</p>
<p>the position returned by the binary search in order of the
length of the longest prefix that matchesq. This can be done
by maintaining two pointers for each sorted order Oσ (one
</p>
<p>moves up and the other down). At each step we move one of
the pointers up or down corresponding to the element with
the longest matching prefix. (Here the length of the longest
matching prefix inOσis computed relative toqwith its bits
</p>
<p>permuted by σ). We examine 2N = O(n1/(1+)) bit vec-
tors in this way. Of all the bit vectors examined, we return
the one that has the smallest Hamming distance toq. The
performance bounds we can prove for this simple scheme
are identical to that proved by Indyk and Motwani for their
scheme. An advantage of this scheme is that we do not need
a reduction to many instances of PLEB for different values
of radius r, i.e. we solve the nearest neighbor problem si-
multaneously for all values of radius r using a single data
structure.
</p>
<p>We outline the main ideas of the analysis. In fact, the
proof follows along similar lines to the proofs of Theorem
5 and Corollary 3 in [31]. Suppose the nearest neighbor of
q is at a Hamming distance ofr from q. Set p1 = 1−<sub>d</sub>r,
</p>
<p>p2 = 1− r(1+)<sub>d</sub> andk = log1/p2n. Let ρ=
ln 1/p1
ln 1/p2. Then
</p>
<p>nρ =O(n1/(1+)). We can show that with constant proba-
bility, from amongst N =O(n1/(1+)<sub>) permutations, there</sub>
</p>
</div>
<div data-page='8' style='padding: 20px;margin: 10px auto;max-width: 800px;'><p>Page 8</p><p>exists a permutation such that the nearest neighbor agrees
withpon the firstk coordinates in σ. Further, over all L
permutations, the number of bit vectors that are at Ham-
ming distance of more thanr(1 +) fromqand agree on the
firstkcoordinates is at most 2N with constant probability.
This implies that for this permutationσ, one of the 2Lbit
vectors nearqin the orderingOσ and examined by the algo-
</p>
<p>rithm will be a (1 +)-approximate nearest neighbor. The
probability calculations are similar to those in [31], and we
only sketch the main ideas.
</p>
<p>For any pointq0 <sub>at distance at least</sub><sub>r(1 +</sub><sub>) from</sub> <sub>q, the</sub>
</p>
<p>probability that a random coordinate agrees with q is at
mostp2. Thus the probability that the firstk coordinates
</p>
<p>agree is at most pk2 = n1. For the N permutations, the
</p>
<p>expected number of such points that agree in the first k
coordinates is at most N. The probability that this num-
ber is≤2N is&gt;1/2. Further, for a random permutation
σ, the probability that the nearest neighbor agrees ink co-
ordinates is pk
</p>
<p>1 = n−ρ. Hence the probability that there
</p>
<p>exists one permutation amongst theN =nρpermutations
where the nearest neighbor agrees inkcoordinates is at least
1−(1−n−ρ<sub>)</sub>nρ
</p>
<p>&gt;1/2. This establishes the correctness of
the procedure.
</p>
<p>As we stated earlier, a nice property of this data structure
is that it automatically adjusts to the correct distance r
to the nearest neighbor, i.e. we do not need to maintain
separate data structures for different values ofr.
</p>
<p><b>6.</b> <b>CONCLUSIONS</b>
</p>
<p>We have demonstrated an interesting relationship between
rounding algorithms used for rounding fractional solutions
of LPs and vector solutions of SDPs on the one hand, and
the construction of locality sensitive hash functions for in-
teresting classes of objects, on the other.
</p>
<p>Rounding algorithms yield new constructions of locality
sensitive hash functions that were not known previously.
Conversely (at least in hindsight), locality sensitive hash
functions lead to rounding algorithms (as in the case of min-
wise independent permutations and the uniform metric case
in Kleinberg and Tardos [33]).
</p>
<p>An interesting direction to pursue would be to investigate
the construction of sketching functions that allow one to es-
timate information theoretic measures of distance between
distributions such as the KL-divergence, commonly used in
statistical learning theory. Since the KL-divergence is nei-
ther symmetric nor satisfies triangle inequality, new ideas
would be required in order to design a sketch function to
approximate it. Such a sketch function, if one exists, would
be a very valuable tool in compactly representing complex
distributions.
</p>
<p><b>7.</b> <b>REFERENCES</b>
</p>
<p>[1] N. Alon, P. B. Gibbons, Y. Matias, and M. Szegedy.
Tracking Join and Self-Join Sizes in Limited Storage.
Proc. 18th PODSpp. 10-20, 1999.
</p>
<p>[2] N. Alon, Y. Matias, and M. Szegedy. The Space
Complexity of Approximating the Frequency
Moments.JCSS58(1): 137-147, 1999
</p>
<p>[3] Y. Bartal. Probabilistic approximation of metric
spaces and its algorithmic application.Proc. 37th
FOCS, pages 184–193, 1996.
</p>
<p>[4] Y. Bartal. On approximating arbitrary metrics by tree
metrics. In Proc. 30th STOC, pages 161–168, 1998.
[5] A. Z. Broder. On the resemblance and containment of
</p>
<p>documents. Proc. Compression and Complexity of
SEQUENCES, pp. 21–29. IEEE Computer Society,
1997.
</p>
<p>[6] A. Z. Broder. Filtering near-duplicate documents.
Proc. FUN 98, 1998.
</p>
<p>[7] A. Z. Broder, M. Charikar, A. Frieze, and
M. Mitzenmacher. Min-wise independent
</p>
<p>permutations.Proc. 30th STOC, pp. 327–336, 1998.
[8] A. Z. Broder, S. C. Glassman, M. S. Manasse, and
</p>
<p>G. Zweig. Syntactic clustering of the Web.Proc. 6th
Int’l World Wide Web Conference, pp. 391–404, 1997.
[9] A. Z. Broder, R. Krauthgamer, and M. Mitzenmacher.
</p>
<p>Improved classification via connectivity information.
Proc. 11th SODA, pp. 576-585, 2000.
</p>
<p>[10] G. Calinescu, H. J. Karloff, and Y. Rabani.
Approximation algorithms for the 0-extension
problem.Proc. 11th SODA, pp. 8-16, 2000.
[11] C. Chekuri, S. Khanna, J. Naor, and L. Zosin.
</p>
<p>Approximation algorithms for the metric labeling
problem via a new linear programming formulation.
Proc. 12th SODA, pp. 109-118, 2001.
</p>
<p>[12] Z. Chen, H. V. Jagadish, F. Korn, N. Koudas,
S. Muthukrishnan, R. T. Ng, and D. Srivastava.
Counting Twig Matches in a Tree.Proc. 17th ICDE
pp. 595-604. 2001.
</p>
<p>[13] Z. Chen, F. Korn, N. Koudas, and S. Muthukrishnan.
Selectivity Estimation for Boolean Queries.Proc. 19th
PODS, pp. 216-225, 2000.
</p>
<p>[14] E. Cohen, M. Datar, S. Fujiwara, A. Gionis, P. Indyk,
R. Motwani, J. D. Ullman, and C. Yang. Finding
Interesting Associations without Support Pruning.
Proc. 16th ICDEpp. 489-499, 2000.
</p>
<p>[15] S. Cohen and L. Guibas. The Earth Mover’s Distance
under Transformation Sets.Proc. 7th IEEE Intnl.
Conf. Computer Vision, 1999.
</p>
<p>[16] S. Cohen and L. Guibas. The Earth Mover’s Distance:
Lower Bounds and Invariance under Translation.
Tech. report STAN-CS-TR-97-1597, Dept. of
Computer Science, Stanford University, 1997.
[17] L. Engebretsen, P. Indyk and R. O’Donnell.
</p>
<p>Derandomized dimensionality reduction with
applications. To appear inProc. 13th SODA, 2002.
[18] P. B. Gibbons and Y. Matias. Synopsis Data
</p>
<p>Structures for Massive Data Sets.Proc. 10th SODA
pp. 909–910, 1999.
</p>
<p>[19] A. C. Gilbert, Y. Kotidis, S. Muthukrishnan, and
M. J. Strauss. Surfing Wavelets on Streams: One-Pass
Summaries for Approximate Aggregate Queries.Proc.
27th VLDBpp. 79-88, 2001.
</p>
<p>[20] A. C. Gilbert, Y. Kotidis, S. Muthukrishnan, and
M. J. Strauss. QuickSAND: Quick Summary and
Analysis of Network Data. DIMACS Technical Report
2001-43, November 2001.
</p>
<p>[21] A. C. Gilbert, S. Guha, P. Indyk, Y. Kotidis,
S. Muthukrishnan, and M. J. Strauss. Fast,
</p>
<p>Small-Space Algorithms for Approximate Histogram
Maintenance.these proceedings.
</p>
<p>[22] A. Gionis, D. Gunopulos, and N. Koudas. Efficient
</p>
</div>
<div data-page='9' style='padding: 20px;margin: 10px auto;max-width: 800px;'><p>Page 9</p><p>and Tunable Similar Set Retrieval.Proc. SIGMOD
Conference2001.
</p>
<p>[23] A. Gionis, P. Indyk, and R. Motwani. Similarity
Search in High Dimensions via Hashing.Proc. 25th
VLDBpp. 518-529, 1999.
</p>
<p>[24] M. X. Goemans and D. P. Williamson. Improved
Approximation Algorithms for Maximum Cut and
Satisfiability Problems Using Semidefinite
Programming.JACM42(6): 1115-1145, 1995.
</p>
<p>[25] S. Guha, N. Mishra, R. Motwani, and L. O’Callaghan.
Clustering data streams.Proc. 41st FOCS, pp.
359–366, 2000.
</p>
<p>[26] A. Gupta and ´Eva Tardos. A constant factor
approximation algorithm for a class of classification
problems.Proc. 32nd STOC, pp. 652–658, 2000.
[27] T. H. Haveliwala, A. Gionis, and P. Indyk. Scalable
</p>
<p>Techniques for Clustering the Web.Proc. 3rd WebDB,
pp. 129-134, 2000.
</p>
<p>[28] P. Indyk. A small approximately min-wise
independent family of hash functions.Proc. 10th
SODA, pp. 454–456, 1999.
</p>
<p>[29] P. Indyk. On approximate nearest neighbors in
non-Euclidean spaces.Proc. 40th FOCS, pp. 148-155,
1999.
</p>
<p>[30] P. Indyk. Stable Distributions, Pseudorandom
Generators, Embeddings and Data Stream
Computation.Proc. 41st FOCS, 189-197, 2000.
[31] Indyk, P., Motwani, R. Approximate nearest
</p>
<p>neighbors: towards removing the curse of
</p>
<p>dimensionality.Proc. 30th STOCpp. 604–613, 1998.
[32] P. Indyk, R. Motwani, P. Raghavan, and S. Vempala.
</p>
<p>Locality-Preserving Hashing in Multidimensional
Spaces.Proc. 29th STOC, pp. 618–625, 1997.
</p>
<p>[33] J. M. Kleinberg and ´Eva Tardos Approximation
Algorithms for Classification Problems with Pairwise
Relationships: Metric Labeling and Markov Random
Fields.Proc. 40th FOCS, pp. 14–23, 1999.
</p>
<p>[34] N. Linial and O. Sasson. Non-Expansive Hashing.
Combinatorica18(1): 121-132, 1998.
</p>
<p>[35] N. Nisan. Pseudorandom sequences for space bounded
computations.Combinatorica, 12:449–461, 1992.
[36] Y. Rubner. Perceptual Metrics for Image Database
</p>
<p>Navigation. Phd Thesis, Stanford University, May
1999
</p>
<p>[37] Y. Rubner, L. J. Guibas, and C. Tomasi. The Earth
Mover’s Distance, Multi-Dimensional Scaling, and
Color-Based Image Retrieval. Proc. of the ARPA
Image Understanding Workshop, pp. 661-668, 1997.
[38] Y. Rubner, C. Tomasi. Texture Metrics.Proc. IEEE
</p>
<p>International Conference on Systems, Man, and
Cybernetics, 1998, pp. 4601-4607.
</p>
<p>[39] Y. Rubner, C. Tomasi, and L. J. Guibas. A Metric for
Distributions with Applications to Image Databases.
Proc. IEEE Int. Conf. on Computer Vision, pp. 59-66,
1998.
</p>
<p>[40] Y. Rubner, C. Tomasi, and L. J. Guibas. The Earth
Mover’s Distance as a Metric for Image Retrieval.
Tech. Report STAN-CS-TN-98-86, Dept. of Computer
Science, Stanford University, 1998.
</p>
<p>[41] M. Ruzon and C. Tomasi. Color edge detection with
the compass operator. Proc. IEEE Conf. Computer
Vision and Pattern Recognition, 2:160-166, 1999.
[42] M. Ruzon and C. Tomasi. Corner detection in
</p>
<p>textured color images.Proc. IEEE Int. Conf.
Computer Vision, 2:1039-1045, 1999.
</p>
</div></body>